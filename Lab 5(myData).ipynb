{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5b1f637",
      "metadata": {
        "id": "d5b1f637"
      },
      "source": [
        "# Lab 5 for Big Data programming.\n",
        "# Apache Spark Machine Learning using Dataframes in Google Colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7739c1ee",
      "metadata": {
        "id": "7739c1ee"
      },
      "source": [
        "# 1.\tSetup an Apache Spark instance in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f9f2c8c2",
      "metadata": {
        "id": "f9f2c8c2"
      },
      "outputs": [],
      "source": [
        "# Run once.\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "#Run Once\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10f08d4",
      "metadata": {
        "id": "b10f08d4"
      },
      "source": [
        "# 2.\tCreate a Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "386acf61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "386acf61",
        "outputId": "6c9e2ec9-375a-495c-f681-f8bf948cb429"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f47bc6aacd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4f89c4e7be90:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad50a00a",
      "metadata": {
        "id": "ad50a00a"
      },
      "source": [
        "# 3.\tDownload the Iris dataset and another dataset of your choosing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AXpWVfdQGd9m"
      },
      "id": "AXpWVfdQGd9m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\" -O sample_data/adult.data\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/\" -O sample_data/forestfires.csv\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/trains/\" -O sample_data/trains-original.data\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/flags/\" -O sample_data/flag.data\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\" -O sample_data/iris.data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2ab683-5727-461f-fd32-c1ce22e710bc",
        "id": "stP1qQ6QGeYR"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 15:30:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 701 [text/html]\n",
            "Saving to: ‘sample_data/adult.data’\n",
            "\n",
            "\rsample_data/adult.d   0%[                    ]       0  --.-KB/s               \rsample_data/adult.d 100%[===================>]     701  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-21 15:30:37 (23.5 MB/s) - ‘sample_data/adult.data’ saved [701/701]\n",
            "\n",
            "--2022-03-21 15:30:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 649 [text/html]\n",
            "Saving to: ‘sample_data/iris.data’\n",
            "\n",
            "sample_data/iris.da 100%[===================>]     649  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-21 15:30:37 (46.8 MB/s) - ‘sample_data/iris.data’ saved [649/649]\n",
            "\n"
          ]
        }
      ],
      "id": "stP1qQ6QGeYR"
    },
    {
      "cell_type": "markdown",
      "id": "a215916e",
      "metadata": {
        "id": "a215916e"
      },
      "source": [
        "# 4.\tImport the Iris dataset into a dataframe and insert screenshot of df.show()command output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "51f4e094",
      "metadata": {
        "id": "51f4e094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "e9b785db-d410-4ede-c679-1f8f7ad8ddf5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-81b99180cea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_iris1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/iris.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SepalLength\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SepalWidth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PetalLength\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PetalWidth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \"\"\"\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (1): _c0\nNew column names (5): SepalLength, SepalWidth, PetalLength, PetalWidth, Class"
          ]
        }
      ],
      "source": [
        "df_iris1 = spark.read.csv('sample_data/iris.data', inferSchema=True)\\\n",
        ".toDF(\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\",\"Class\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_iris.printSchema()"
      ],
      "metadata": {
        "id": "fnU-TbGOFb6j",
        "outputId": "6fb2c803-7c09-4859-9854-3ebfe6230857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fnU-TbGOFb6j",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- SepalLength: double (nullable = true)\n",
            " |-- SepalWidth: double (nullable = true)\n",
            " |-- PetalLength: double (nullable = true)\n",
            " |-- PetalWidth: double (nullable = true)\n",
            " |-- Class: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult = spark.read.csv('sample_data/adult.data', inferSchema=True)\\\n",
        ".toDF(\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"class\")\n"
      ],
      "metadata": {
        "id": "oCaGhJVYFpT5",
        "outputId": "92c02322-7dbb-4359-c2ac-e5e43082c8d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "id": "oCaGhJVYFpT5",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d7402d343839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_adult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/adult.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"workclass\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fnlwgt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"education\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"education-num\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"marital-status\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"occupation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"relationship\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"race\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"capital-gain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"capital-loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hours-per-week\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"native-country\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \"\"\"\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (1): _c0\nNew column names (15): age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, class"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.printSchema()"
      ],
      "metadata": {
        "id": "EMyMta_kFpcU",
        "outputId": "3ec47034-3bd7-42b7-8e63-af30e0b1f5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "id": "EMyMta_kFpcU",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b35a4991e0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_adult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_adult' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bee4106",
      "metadata": {
        "id": "4bee4106"
      },
      "source": [
        "# 5.\tSpark ML can only deal with one features column - so we need to vectorise the multiple columns into one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56081b8a",
      "metadata": {
        "id": "56081b8a"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28cf0ae0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28cf0ae0",
        "outputId": "b623949e-6c22-4683-a0b2-17edcfdb19de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "|SepalLength|SepalWidth|PetalLength|PetalWidth|      Class|ClassIndex|         features|\n",
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|       0.0|[5.1,3.5,1.4,0.2]|\n",
            "|        4.9|       3.0|        1.4|       0.2|Iris-setosa|       0.0|[4.9,3.0,1.4,0.2]|\n",
            "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|       0.0|[4.7,3.2,1.3,0.2]|\n",
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vector_assembler=VectorAssembler(\\\n",
        "inputCols=[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"],\\\n",
        "outputCol=\"features\")\n",
        "df=vector_assembler.transform(df)\n",
        "df.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a0e244",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0a0e244",
        "outputId": "903facfa-197b-4e94-a095-b4cac2faf10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------------+\n",
            "|      Class|ClassIndex|         features|\n",
            "+-----------+----------+-----------------+\n",
            "|Iris-setosa|       0.0|[5.1,3.5,1.4,0.2]|\n",
            "|Iris-setosa|       0.0|[4.9,3.0,1.4,0.2]|\n",
            "|Iris-setosa|       0.0|[4.7,3.2,1.3,0.2]|\n",
            "+-----------+----------+-----------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_temp=df.drop(\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\")\n",
        "df_temp.show(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a56a38",
      "metadata": {
        "id": "31a56a38"
      },
      "source": [
        "# 6.\tThe final data preparation step is to index the Class column - to use numeric rather than text values - run the following command and display your output of Class, features & ClassIndex columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2a1ef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "da2a1ef0",
        "outputId": "5477ae84-53de-4377-bce8-5f0a3f1da28d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-c67e83785c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ml_indexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ClassIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_indexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Output column ClassIndex already exists."
          ]
        }
      ],
      "source": [
        "#from pyspark.ml.feature import StringIndexer\n",
        "#l_indexer=StringIndexer(inputCol=\"Class\", outputCol=\"ClassIndex\")\n",
        "#df = l_indexer.fit(df).transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c31775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25c31775",
        "outputId": "8f1ca8f2-1617-4c8b-ded0-c56eb3aac529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "|SepalLength|SepalWidth|PetalLength|PetalWidth|      Class|ClassIndex|         features|\n",
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|       0.0|[5.1,3.5,1.4,0.2]|\n",
            "|        4.9|       3.0|        1.4|       0.2|Iris-setosa|       0.0|[4.9,3.0,1.4,0.2]|\n",
            "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|       0.0|[4.7,3.2,1.3,0.2]|\n",
            "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|       0.0|[4.6,3.1,1.5,0.2]|\n",
            "|        5.0|       3.6|        1.4|       0.2|Iris-setosa|       0.0|[5.0,3.6,1.4,0.2]|\n",
            "|        5.4|       3.9|        1.7|       0.4|Iris-setosa|       0.0|[5.4,3.9,1.7,0.4]|\n",
            "|        4.6|       3.4|        1.4|       0.3|Iris-setosa|       0.0|[4.6,3.4,1.4,0.3]|\n",
            "|        5.0|       3.4|        1.5|       0.2|Iris-setosa|       0.0|[5.0,3.4,1.5,0.2]|\n",
            "|        4.4|       2.9|        1.4|       0.2|Iris-setosa|       0.0|[4.4,2.9,1.4,0.2]|\n",
            "|        4.9|       3.1|        1.5|       0.1|Iris-setosa|       0.0|[4.9,3.1,1.5,0.1]|\n",
            "+-----------+----------+-----------+----------+-----------+----------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07307297",
      "metadata": {
        "id": "07307297"
      },
      "source": [
        "# 7.\tSplit your data into training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dcd5495",
      "metadata": {
        "id": "9dcd5495"
      },
      "outputs": [],
      "source": [
        "(trainingData,testData) = df.randomSplit([0.7,0.3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdba3ce0",
      "metadata": {
        "id": "cdba3ce0"
      },
      "source": [
        "# 8.\tDecision Tree Classifier \n",
        "## Specify the DecisionTreeClassifier and train the model on your training dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51aa78dc",
      "metadata": {
        "id": "51aa78dc"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier \n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ea2a05",
      "metadata": {
        "id": "e9ea2a05"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(labelCol=\"ClassIndex\",featuresCol=\"features\")\n",
        "model=dt.fit(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4692ce",
      "metadata": {
        "id": "db4692ce"
      },
      "source": [
        "# 9.\tTest your model with your test dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1487d4",
      "metadata": {
        "id": "9e1487d4"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(testData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b91670",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94b91670",
        "outputId": "bb56279c-d1e0-4129-93f2-17ed56094e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|prediction|ClassIndex|\n",
            "+----------+----------+\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       1.0|       1.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       1.0|       1.0|\n",
            "|       1.0|       1.0|\n",
            "+----------+----------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions.select(\"prediction\",\"ClassIndex\").show(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43df52f9",
      "metadata": {
        "id": "43df52f9"
      },
      "source": [
        "# 10.\tRun an evaluator function to show the accuracy of your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61f6143",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a61f6143",
        "outputId": "cf4ea5b2-a969-4315-8346-68366f7c8988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error = 0.047619\n",
            "Test Set accuracy = 0.9523809523809523\n"
          ]
        }
      ],
      "source": [
        "evaluator= MulticlassClassificationEvaluator(\\\n",
        "labelCol=\"ClassIndex\", predictionCol=\"prediction\",\\\n",
        "metricName=\"accuracy\")\n",
        "accuracy=evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "print(\"Test Set accuracy = \" +str(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b157244e",
      "metadata": {
        "id": "b157244e"
      },
      "source": [
        "# 11.\tRandom Forest Classifier\n",
        "\n",
        "## Specify the RandomForestClassifier, train the model on your training dataset, predict using your test dataset, and run an evaluator to test accuracy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3578121",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3578121",
        "outputId": "7185ce11-39ed-485c-d222-5c923b2ee819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|prediction|ClassIndex|\n",
            "+----------+----------+\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       1.0|       1.0|\n",
            "+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier \n",
        "rf=RandomForestClassifier(labelCol=\"ClassIndex\",\\\n",
        "featuresCol=\"features\",numTrees=10)\n",
        "model=rf.fit(trainingData)\n",
        "predictions=model.transform(testData)\n",
        "predictions.select(\"prediction\",\"ClassIndex\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443d95e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "443d95e7",
        "outputId": "42e99c83-9b3e-4d2f-ddb5-377f4480355f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error = 0.0714286\n",
            "Test Set accuracy = 0.9285714285714286\n"
          ]
        }
      ],
      "source": [
        "evaluator= \\\n",
        "MulticlassClassificationEvaluator(labelCol=\"ClassIndex\",\\\n",
        "predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy=evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "print(\"Test Set accuracy = \" +str(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbecf19",
      "metadata": {
        "id": "0bbecf19"
      },
      "source": [
        "# 12.\tNaive Bayes Classifier\n",
        "## Specify the NaiveBayes classifier, train the model on your training dataset, predict using your test dataset, and run an evaluator to test accuracy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c51651",
      "metadata": {
        "id": "74c51651"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import NaiveBayes \n",
        "nb=NaiveBayes(labelCol=\"ClassIndex\",\\\n",
        "featuresCol=\"features\",smoothing=1.0,\\\n",
        "modelType=\"multinomial\")\n",
        "model=nb.fit(trainingData)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be780693",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be780693",
        "outputId": "84e86794-3b74-41ed-9120-efa8bb0800cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------+--------------------+----------+\n",
            "|          Class|ClassIndex|         probability|prediction|\n",
            "+---------------+----------+--------------------+----------+\n",
            "|    Iris-setosa|       0.0|[0.67469033867968...|       0.0|\n",
            "|    Iris-setosa|       0.0|[0.72261167673864...|       0.0|\n",
            "|    Iris-setosa|       0.0|[0.65606788034250...|       0.0|\n",
            "|    Iris-setosa|       0.0|[0.71533253396407...|       0.0|\n",
            "|Iris-versicolor|       1.0|[0.11076393934278...|       1.0|\n",
            "+---------------+----------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions=model.transform(testData)\n",
        "predictions.select(\"Class\",\"ClassIndex\",\n",
        "\"probability\",\"prediction\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0965acd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0965acd9",
        "outputId": "a99c2021-321a-4cae-b4d9-40823ff9055f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error = 0.0714286\n",
            "Test Set accuracy = 0.9285714285714286\n"
          ]
        }
      ],
      "source": [
        "evaluator= \\\n",
        "MulticlassClassificationEvaluator(labelCol=\"ClassIndex\",\\\n",
        "predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy=evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "print(\"Test Set accuracy = \" +str(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "da328635",
      "metadata": {
        "id": "da328635"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\" -O sample_data/adult.data\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/\" -O sample_data/forestfires.csv\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/trains/\" -O sample_data/trains-original.data\n",
        "#!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/flags/\" -O sample_data/flag.data\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\" -O sample_data/iris.data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dkg2F8JG6Qa",
        "outputId": "561e7af5-e6c4-455d-b357-54cca8371905"
      },
      "id": "0dkg2F8JG6Qa",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 15:24:09--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 649 [text/html]\n",
            "Saving to: ‘sample_data/iris.data’\n",
            "\n",
            "\rsample_data/iris.da   0%[                    ]       0  --.-KB/s               \rsample_data/iris.da 100%[===================>]     649  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-21 15:24:09 (25.3 MB/s) - ‘sample_data/iris.data’ saved [649/649]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df6 = spark.read.csv('sample_data/iris.data', header=False,sep=\",\",inferSchema=True)\n",
        "df6.printSchema()"
      ],
      "metadata": {
        "id": "PuFJYGb3E6iI",
        "outputId": "d017a8a0-b7a9-48bd-ca6f-aabaeebcbc1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PuFJYGb3E6iI",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nMvOLlqoE6le"
      },
      "id": "nMvOLlqoE6le",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf72667d-9c1c-4c3b-bf72-bf3542eaafa9",
        "id": "QX3_0QYUEu1r"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-20 20:51:18--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4551 (4.4K) [application/x-httpd-php]\n",
            "Saving to: ‘sample_data/iris.data’\n",
            "\n",
            "sample_data/iris.da 100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-20 20:51:19 (143 MB/s) - ‘sample_data/iris.data’ saved [4551/4551]\n",
            "\n",
            "--2022-03-20 20:51:19--  https://archive.ics.uci.edu/ml/datasets/Adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘sample_data/adult.data’\n",
            "\n",
            "sample_data/adult.d     [ <=>                ]  41.33K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-20 20:51:19 (307 KB/s) - ‘sample_data/adult.data’ saved [42320]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" -O sample_data/iris.data\n",
        "!wget \"https://archive.ics.uci.edu/ml/datasets/Adult/adult.data\" -O sample_data/adult.data\n"
      ],
      "id": "QX3_0QYUEu1r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf72667d-9c1c-4c3b-bf72-bf3542eaafa9",
        "id": "yfsB0RMmEthK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-20 20:51:18--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4551 (4.4K) [application/x-httpd-php]\n",
            "Saving to: ‘sample_data/iris.data’\n",
            "\n",
            "sample_data/iris.da 100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-20 20:51:19 (143 MB/s) - ‘sample_data/iris.data’ saved [4551/4551]\n",
            "\n",
            "--2022-03-20 20:51:19--  https://archive.ics.uci.edu/ml/datasets/Adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘sample_data/adult.data’\n",
            "\n",
            "sample_data/adult.d     [ <=>                ]  41.33K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-03-20 20:51:19 (307 KB/s) - ‘sample_data/adult.data’ saved [42320]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" -O sample_data/iris.data\n",
        "!wget \"https://archive.ics.uci.edu/ml/datasets/Adult/adult.data\" -O sample_data/adult.data\n"
      ],
      "id": "yfsB0RMmEthK"
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = spark.read.csv('sample_data/flag.data', header=False,sep=\",\",inferSchema=True)\n",
        "df5.printSchema()"
      ],
      "metadata": {
        "id": "ZVBWub5dDhhp",
        "outputId": "e36a1aa4-2fe6-4c6e-afd3-ce291670b571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZVBWub5dDhhp",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gNHKzybwDhmc"
      },
      "id": "gNHKzybwDhmc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PBkabdZJDhqq"
      },
      "id": "PBkabdZJDhqq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = spark.read.csv('sample_data/flag.data', inferSchema=True)\\\n",
        ".toDF(\"name\",\"landmass\",\"zone\",\"area\",\"population\",\"language\",\"religion\",\"bars\",\"stripes\",\"colours\",\"red\",\"green\",\"blue\",\"gold\",\"white\",\"black\",\"orange\",\"mainhue\",\"circles\",\"crosses\",\"saltires\",\"quarters\",\"sunstars\",\"crescent\",\"triangle\",\"icon\",\"animate\",\"text\",\"topleft\",\"botright\")"
      ],
      "metadata": {
        "id": "FYiv6dzAAxTe",
        "outputId": "91860c5a-7df4-478c-ef75-b7fa0efeccf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "id": "FYiv6dzAAxTe",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-02ae0889f44b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/flag.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"landmass\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"zone\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"area\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"population\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"religion\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bars\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stripes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"colours\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gold\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"orange\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mainhue\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"circles\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"crosses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"saltires\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"quarters\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sunstars\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"crescent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"triangle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"icon\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"animate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"topleft\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"botright\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \"\"\"\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (1): _c0\nNew column names (30): name, landmass, zone, area, population, language, religion, bars, stripes, colours, red, green, blue, gold, white, black, orange, mainhue, circles, crosses, saltires, quarters, sunstars, crescent, triangle, icon, animate, text, topleft, botright"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y0wGqKujAxaa"
      },
      "id": "y0wGqKujAxaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HZrn6ULnAxeq"
      },
      "id": "HZrn6ULnAxeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = spark.read.csv('sample_data/forestfires.csv', header=False, sep=\",\",inferSchema=True)\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "Rcxl7vad9usO",
        "outputId": "f75b4830-d1a3-467c-b779-7911560d9a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Rcxl7vad9usO",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 _c0|\n",
            "+--------------------+\n",
            "|<!DOCTYPE HTML PU...|\n",
            "|              <html>|\n",
            "|              <head>|\n",
            "|  <title>Index of...|\n",
            "|             </head>|\n",
            "|              <body>|\n",
            "|<h1>Index of /ml/...|\n",
            "|<ul><li><a href=\"...|\n",
            "|<li><a href=\"fore...|\n",
            "|<li><a href=\"fore...|\n",
            "|               </ul>|\n",
            "|<address>Apache/2...|\n",
            "|      </body></html>|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RvZ5a1k19vC5"
      },
      "id": "RvZ5a1k19vC5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = spark.read.csv('sample_data/adult.data', inferSchema=True)\\\n",
        ".toDF(\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"class\")\n",
        "\n",
        "\n",
        "spark.read.csv('sample_data/adult.data', inferSchema=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "i-blw9-SHPVJ",
        "outputId": "659fa8b7-cdc2-47f3-a10a-491a885b4026"
      },
      "id": "i-blw9-SHPVJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-00b008e26520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/adult.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"workclass\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fnlwgt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"education\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"education-num\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"marital-status\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"occupation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"relationship\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"race\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sex\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"capital-gain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"capital-loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hours-per-week\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"native-country\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/adult.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \"\"\"\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (1): _c0\nNew column names (15): age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, class"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_adult.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pUacsE_GpUD",
        "outputId": "e279a760-2b54-4596-c5be-04303f5cda8a"
      },
      "id": "-pUacsE_GpUD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 _c0|\n",
            "+--------------------+\n",
            "|<!DOCTYPE HTML PU...|\n",
            "|              <html>|\n",
            "|              <head>|\n",
            "+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v1Vjw9T5G5dr"
      },
      "id": "v1Vjw9T5G5dr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Lab 5.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}